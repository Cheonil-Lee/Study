# 목표

1. 원리(어떻게 돌아가는 건지 설명)
2. 코드 돌려보기(예제 찾아서) → ~~ipython 환경 구축~~ (그냥 코랩으로 돌리기)
3. 전체적인 과정 정리
4. 절대 잊어먹지 말기, 구분선 해서 오늘 공부한 거 정리하기

논문 보고 싶으면 리스트업 하기 or 리뷰한 거 찾아보기

https://pseudo-lab.github.io/Tutorial-Book/chapters/object-detection/Ch1-Object-Detection.html

## 1. 객체 탐지 소개

물체가 있는 위치를 탐지함과 동시에 해당 물체가 무엇인지(강아지) 분류한다면 해당 모델은 객체 탐지 모델이다.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/babee6b0-671a-489b-911d-90c07cf8e8dc/da319c51-528f-4a73-a8e8-1d1b10a6051e/image.png)

                      <이미지 분류 모델>                                                 <객체 탐지 모델>

## 1. 1. 바운딩 박스

- 특정 사물을 탐지하여 모델을 효율적으로 학습 할 수 있도록 도움을 주는 방법이다.
- 타겟 위치를 특정하기 위해 사용된다.
- 타겟 위치를 X와 Y축을 이용하여 사각형으로 표현한다.

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/babee6b0-671a-489b-911d-90c07cf8e8dc/91c36563-dc9b-42cf-b7a5-8b38ef29f67b/image.png)

- 바운딩 박스 값은 (X 최소값, Y 최소값, X 최대값, Y 최대값)으로 표현한다.
- 그림 X와 Y의 최소값과 최대값 사이의 면적을 바운딩 박스 영역으로 잡는다.
- 위의 X, Y 값은 픽셀값으로 효율적인 연산을 위해서는 최대값 1로 변환을 해야 한다.
- 그림의 X, Y 값은 각각 X의 최대값 971, Y의 최대값 547을 나눈 값이다.
- X의 최소값은 640에서 971을 나누면 0.66이 되는 것이다.
- 이렇게 분수화는 효율적인 연산을 위한 과정이라고 볼 수 있지만, 필수적인 과정은 아니다.

https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection

바운딩 영역 픽셀값으로 지정

https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection

바운딩 영역 백분위로 지정

## 2. 객체 탐지 모델 형태

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/babee6b0-671a-489b-911d-90c07cf8e8dc/dfebc803-e92a-4c9a-8139-a261e2d5441e/image.png)

<객체 탐지 알고리즘 타임라인>

### Classification

특정 물체에 대해 어떤 물체인지 분류를 하는 것

https://medium.com/@hugmanskj/classification-overview-bd35005fc7a0

### Region Proposal

물체가 있을만한 영역을 빠르게 찾아내는 알고리즘

https://velog.io/@suminwooo/RPNRegion-Proposal-Network-정리

### Two-Stage Detector모델

 Classification, Regional Proposal을 순차적으로 수행하여 객체를 검출하는 정확도 측면에서는 좋은 성능을 냈지만, 예측 속도가 느려 실시간 탐지에는 제한되었음

### One-Stage Detector모델

Two-Stage 모델이 가진 실시간 탐지 제한을 해결하기 위해 Classification과 Region Propsal을 동시에 하는 One-Stage Detector가 제안되었음

## **1.2.1. One-Stage Detector**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/babee6b0-671a-489b-911d-90c07cf8e8dc/926cc298-bddc-49c9-91b8-65f0937d69c4/image.png)

- Classification, Regional Proposal을 동시에 수행하여 결과를 얻는 방법
- 이미지를 모델에 입력 후, Conv Layer를 사용하여 이미지 특징을 추출

https://medium.com/becoming-human/explaining-yolov4-a-one-stage-detector-cdac0826cbd7

## **1.2.2. Two-Stage Detector**

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/babee6b0-671a-489b-911d-90c07cf8e8dc/57597820-fa42-447c-878a-302db70994d5/image.png)

- Classification, Regional Proposal을 순차적으로 수행하여 결과를 얻는 방법

https://medium.com/@kattarajesh2001/object-detection-part-2-two-stage-detectors-r-cnn-fast-r-cnn-faster-r-cnn-026e5fac5dd0

**One-Stage Detector**는 비교적 **빠르지만** **정확도가 낮고**, **Two-Stage Detector**는 비교적 **느리지만 정확도가 높다**.

1.3 모델 구조

Two-Stage Detector: R-CNN, Fast R-CNN, Faster R-CNN 

One-Stage Detector: YOLO, SSD, RetinaNet

1.3.1 R-CNN

https://velog.io/@woojinn8/Object-Detection-1.-R-CNN-리뷰

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/babee6b0-671a-489b-911d-90c07cf8e8dc/4fff7a30-c7ae-4889-8b83-ab93d48bcdd9/image.png)

                                                                             R-CNN 구조

R-CNN은 Selective Search를 이용해 이미지에 대한 후보영역(Region Proposal)을 생성합니다. 

생성된 각 후보영역을 고정된 크기로 wrapping하여 CNN의 input으로 사용합니다. 

CNN에서 나온 Feature map으로 SVM을 통해 분류, Regressor을 통해 Bounding-box를 조정합니다. 

강제로 크기를 맞추기 위한 wrapping으로 이미지의 변형이나 손실이 일어나고 후보영역만큼 CNN을 돌려야하하기 때문에 큰 저장공간을 요구하고 느리다는 단점이 있습니다.

# TMI

yolo는 **Image Segmentation를 안함.**

→ Object Detection로 이미 해서, 느려서 안함

meta-SAM

→ 모든 걸 **Image Segmentation** 하겠다. 근데 빠름

 **Image Segmentation → 과일 멍든거**

→ 경계 파악을 잘한다. 

<경계 파악을 어떻게 하죠? → 영상처리, 경계검출>
